{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Approaches based on a total of 160K reviews:\n",
    "*   Training Data: 120K reviews\n",
    "*   Testing Data: 200 reviews\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages & Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\programdata\\anaconda3\\lib\\site-packages (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "# # For installation via Anaconda: preferred using \"conda\".\n",
    "# # For installation via Google Colab or packages not available in conda: using \"pip\".\n",
    "\n",
    "# # Install the Pandas library (Python Data Analysis Library) via Pip:\n",
    "# # The exclamation mark ! means run it as a shell command rather than a notebook command.\n",
    "# # Google Colab wants an exclamation point before most commands.\n",
    "# \n",
    "# # Install the Pandas library for data manipulation and analysis:\n",
    "# # Use Pandas especially for manipulating numerical tables and time series.\n",
    "# !pip install pandas\n",
    "# \n",
    "# # Install the Numpy library for scientific & mathematical operation:\n",
    "# # Numpy allows to work with multi-dimensional arrays & matrices.\n",
    "# !pip install numpy\n",
    "# \n",
    "# # Install the Beautiful Soup library for scraping data from HTML and XML files:\n",
    "# !pip install beautifulsoup4\n",
    "# \n",
    "# # Install the NLTK library for NLP:\n",
    "# !pip install nltk\n",
    "# # scipy needs to be installed prior to gensim installation.\n",
    "# !pip3 install scipy\n",
    "# !pip install gensim\n",
    "# \n",
    "# # Install the Scikit-learn for supervise & unsupervise learning algorithms:\n",
    "# !pip install scikit-learn\n",
    "# \n",
    "# # Install the Matplotlib library for visualization:\n",
    "# !pip install matplotlib\n",
    "# # Install the Seaborn library for pretty plot:\n",
    "# !pip install seaborn\n",
    "\n",
    "# \n",
    "# Install the Vader Lexicon for Sentiment Analysis:\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Louise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Louise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Louise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Louise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for data preprocessing:\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import NLTK modules:\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK Punkt Sentence Tokenizer that divides a text into a list of sentences:\n",
    "nltk.download('punkt')\n",
    "# Import tokenizer for word and sentence:\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download the NLTK stopword corpus:\n",
    "nltk.download('stopwords')\n",
    "# Import the Stopwords module:\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import PorterStemmer for stemming words:\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Download the NLTK wordnet corpus:\n",
    "nltk.download('wordnet')\n",
    "# Import WordNetLemmatizer for lemmatization:\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download the NLTK vader_lexicon:\n",
    "nltk.download('vader_lexicon')\n",
    "# Import SentimentIntensityAnalyzer from NLTK Vader lexicon:\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "# Import CountVectorizer from Sciket Learn for converting a collection of text documents to a matrix of token counts:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import train_test_split from Sciket Learn for splitting the dataset:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Gaussian Naive Bayes model from Sciket Learn for classification:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Import RandomForestClassifier from Sciket Learn for classification:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import confusion_matrix from Scikit Learn for accuracy evaluation:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import accuracy_score from Scikit Learn for computing the prediction accuracy:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import TfidfVectorizer from Scikit Learn for converting a collection of raw documents to a matrix of TF-IDF features (in percentage):\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Important Kmeans from Scikit Learn for K-means Clustering:\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Import LinearRegression, LogisticRegression from Scikit Learn for Regression modeling:\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import SVM from Sciket Learn for classification:\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# Import the Python built-in RegEx (Regular Expression) module for text cleaning & text search:\n",
    "import re\n",
    "\n",
    "\n",
    "# Import Matplotlib modules:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the output of plotting commands to be displayed inline in the notebook document (E.g., Jupyter, Colob):\n",
    "%matplotlib inline\n",
    "\n",
    "# # Set the interactive plots embedded within the notebook that allow zoom and resize:\n",
    "# %matplotlib notebook\n",
    "\n",
    "# Import the Seaborn library for pretty statiscal visualization:\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-cL6y6JIl6X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QscPPziOIl6c"
   },
   "outputs": [],
   "source": [
    "# Load dataset from a file:\n",
    "text_file = open(r\"train1.ft.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "# Split texts in the file into lines:\n",
    "lines = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600001\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last element in the list \"lines\":\n",
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the last element in the list with pop() by default or pop(-1) and return the element that is removed:\n",
    "lines.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"__label__2 Makes My Blood Run Red-White-And-Blue: I agree that every American should read this book -- and everybody else for that matter. I don't agree that it's scholarly. Rather, it's a joy to read -- easy to understand even for a person with two master's degrees! Between McElroy's chapter on How American Culture was Formed and Ken Burns' Lewis & Clark, I don't know which makes my blood run red-white-and-bluer. And as a child of the anti-establishment `60s, it's done a lot toward helping me understand why we Americans do what we do. It's the best history book I've ever read, the best history course I've ever taken or taught. I'm buying it for my home library for my grandchildren to use as a resource. We're also using it as a resource for a book on urban planning.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dele0Lm5Il6f"
   },
   "outputs": [],
   "source": [
    "# Define the function for replacing label strings with integer 0 or 1:\n",
    "def replace_label(text):\n",
    "    labels = []\n",
    "    \n",
    "    for item in text:\n",
    "        first_ten_chars = item[:10]\n",
    "        if first_ten_chars == '__label__1':\n",
    "            labels.append(int(0))   # 0 for Negative Review: '__label__1'\n",
    "        elif first_ten_chars == '__label__2':\n",
    "            labels.append(int(1))   # 1 for Positive Review: '__label__2'\n",
    "            \n",
    "    return labels\n",
    "\n",
    "labels = replace_label(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600000\n",
      "1\n",
      "1\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0])\n",
    "print(labels[-1])\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "918ayx3kIl6h"
   },
   "outputs": [],
   "source": [
    "# Define the function for removing lable strings in lines:\n",
    "def remove_label(s):\n",
    "    return s[11:]   # The text review starts from index 11 to the last index.\n",
    "\n",
    "lines = [remove_label(s) for s in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600000\n",
      "\n",
      "Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print()\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NOSy1uEIl6j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2725661</td>\n",
       "      <td>this movie sucks: This movie supposedly about ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798719</td>\n",
       "      <td>Good Entertainment: This program a well edited...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242154</td>\n",
       "      <td>Does the job: This hamper does the job in my k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3373098</td>\n",
       "      <td>Buffett Mails it In: Being a huge Buffett fan,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663895</td>\n",
       "      <td>Sharp as a razor... almost.: Wow! My replaceme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label\n",
       "2725661  this movie sucks: This movie supposedly about ...      0\n",
       "1798719  Good Entertainment: This program a well edited...      0\n",
       "1242154  Does the job: This hamper does the job in my k...      1\n",
       "3373098  Buffett Mails it In: Being a huge Buffett fan,...      0\n",
       "1663895  Sharp as a razor... almost.: Wow! My replaceme...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain = pd.DataFrame()\n",
    "dataTrain['text'] = lines\n",
    "dataTrain['label'] = labels\n",
    "\n",
    "dataTrain = dataTrain.sample(n = 120000, random_state = 123)\n",
    "print(len(dataTrain))\n",
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for cleaning texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Define the function for lemmatizing texts:\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# Define the function for changing to lowercase, removing punctuation and stopwords:  \n",
    "def clean_text(text):\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace('-', ' ')\n",
    "    text = text.str.split(' ')\n",
    "    text = text.apply(lambda x: [item for item in x if item not in stop])\n",
    "    text = text.apply(', '.join)\n",
    "    text = text.str.replace('[{}]'.format(string.punctuation), '')\n",
    "    text = text.apply(lemmatize_text)\n",
    "    text = text.apply(', '.join)\n",
    "    text = text.str.replace('[{}]'.format(string.punctuation), '')\n",
    "    text = text.str.replace('\\\\', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain['text'] = clean_text(dataTrain['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 120K Reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2725661</td>\n",
       "      <td>movie suck movie supposedly michael suck crapp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798719</td>\n",
       "      <td>good entertainment program well edited visuall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242154</td>\n",
       "      <td>job hamper job kid room hold two three load de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3373098</td>\n",
       "      <td>buffett mail in huge buffett fan bought this u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663895</td>\n",
       "      <td>sharp razor almost wow replacement sharp cut s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label\n",
       "2725661  movie suck movie supposedly michael suck crapp...      0\n",
       "1798719  good entertainment program well edited visuall...      0\n",
       "1242154  job hamper job kid room hold two three load de...      1\n",
       "3373098  buffett mail in huge buffett fan bought this u...      0\n",
       "1663895  sharp razor almost wow replacement sharp cut s...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Data: 120K Reviews\")\n",
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QscPPziOIl6c"
   },
   "outputs": [],
   "source": [
    "# Load dataset from a file:\n",
    "text_file = open(r\"test1.ft.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "# Split texts in the file into lines:\n",
    "lines = text_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the last element in the list \"lines\":\n",
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the last element in the list with pop() by default or pop(-1) and return the element that is removed:\n",
    "lines.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"__label__1 Comedy Scene, and Not Heard: This DVD will be a disappointment if you get it hoping to see some substantial portion of the acts of the various comics listed on the cover. All you get here are snippets of performance, at best. The rest is just loose-leaf reminiscence about the good old days in Boston, in the early 80's, when a lot of comics were hanging out together and getting their start.It's like a frat house reunion. There's a lot of lame nostalgia. There are quite a few guffaws recalling jokes (practical and otherwise)perpetrated - back then. But you had to have been there to appreciate all the basically good ol' boy camaraderie. If you weren't actually a part of that scene, all this joshing and jostling will fall flat.If you want to actually hear some of these comics' routines - you will have to look elsewhere.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dele0Lm5Il6f"
   },
   "outputs": [],
   "source": [
    "# # Define the function for replacing label strings with integer 0 or 1:\n",
    "# def replace_label(text):\n",
    "#     labels = []\n",
    "    \n",
    "#     for item in text:\n",
    "#         first_ten_chars = item[:10]\n",
    "#         if first_ten_chars == '__label__1':\n",
    "#             labels.append(int(0))   # 0 for Negative Review: '__label__1'\n",
    "#         elif first_ten_chars == '__label__2':\n",
    "#             labels.append(int(1))   # 1 for Positive Review: '__label__2'\n",
    "            \n",
    "#     return labels\n",
    "\n",
    "labels = replace_label(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "1\n",
      "0\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0])\n",
    "print(labels[-1])\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "918ayx3kIl6h"
   },
   "outputs": [],
   "source": [
    "# Define the function for removing lable strings in lines:\n",
    "def remove_label(s):\n",
    "    return s[11:]   # The text review starts from index 11 to the last index.\n",
    "\n",
    "lines = [remove_label(s) for s in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "\n",
      "Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print()\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1NOSy1uEIl6j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>333305</td>\n",
       "      <td>Confused: I have been a science fiction/fantas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27936</td>\n",
       "      <td>What a SORRY A$$ way to go out!: Since this is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17999</td>\n",
       "      <td>If I had my way, I'd have all of you shot: I l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124332</td>\n",
       "      <td>Super Fun for My Super Heroes!: You cannot eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303110</td>\n",
       "      <td>Extremely Poor Quality: This bit set is absolu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "333305  Confused: I have been a science fiction/fantas...      0\n",
       "27936   What a SORRY A$$ way to go out!: Since this is...      0\n",
       "17999   If I had my way, I'd have all of you shot: I l...      1\n",
       "124332  Super Fun for My Super Heroes!: You cannot eve...      1\n",
       "303110  Extremely Poor Quality: This bit set is absolu...      0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTest = pd.DataFrame()\n",
    "dataTest['text'] = lines\n",
    "dataTest['label'] = labels\n",
    "\n",
    "dataTest = dataTest.sample(n = 40000, random_state = 456)\n",
    "print(len(dataTest))\n",
    "dataTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = stopwords.words('english')\n",
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# # Define the function for lemmatizing texts:\n",
    "# def lemmatize_text(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "# # Define the function for changing to lowercase, removing punctuation and stopwords:  \n",
    "# def clean_text(text):\n",
    "#     text = text.str.lower()\n",
    "#     text = text.str.replace('-', ' ')\n",
    "#     text = text.str.split(' ')\n",
    "#     text = text.apply(lambda x: [item for item in x if item not in stop])\n",
    "#     text = text.apply(', '.join)\n",
    "#     text = text.str.replace('[{}]'.format(string.punctuation), '')\n",
    "#     text = text.apply(lemmatize_text)\n",
    "#     text = text.apply(', '.join)\n",
    "#     text = text.str.replace('[{}]'.format(string.punctuation), '')\n",
    "#     text = text.str.replace('\\\\', ' ')\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTest['text'] = clean_text(dataTest['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 40K Reviews\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>333305</td>\n",
       "      <td>confused science fictionfantasy book fan long ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27936</td>\n",
       "      <td>sorry a way go out since supposedly master p l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17999</td>\n",
       "      <td>way id shot love the wall movie album story cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124332</td>\n",
       "      <td>super fun super hero cannot even imagine excit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303110</td>\n",
       "      <td>extremely poor quality bit set absolutely awfu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "333305  confused science fictionfantasy book fan long ...      0\n",
       "27936   sorry a way go out since supposedly master p l...      0\n",
       "17999   way id shot love the wall movie album story cl...      1\n",
       "124332  super fun super hero cannot even imagine excit...      1\n",
       "303110  extremely poor quality bit set absolutely awfu...      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Data: 40K Reviews\")\n",
    "dataTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis via Lexicon: VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SentimentIntensityAnalyzer from NLTK Vader lexicon:\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vader sentiment analyzer:\n",
    "vaderAnalyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define the function of review rate:\n",
    "def sentiment_analyzer_scores(text):\n",
    "  score = vaderAnalyzer.polarity_scores(text)\n",
    "  print(text)\n",
    "  print(score)\n",
    "  print()\n",
    "\n",
    "  print(\"Overall, the review is rated as: \")\n",
    "\n",
    "  if score['compound'] <= -0.05:\n",
    "    print(\"Negative (Label: 0)\")\n",
    "\n",
    "  elif score['compound'] >= 0.05:\n",
    "    print(\"Positive (Label: 1)\")\n",
    "\n",
    "  else:\n",
    "    print(\"Neutral (No Label)\")\n",
    "\n",
    "  print()\n",
    "  print(\"-------------------------------------\")\n",
    "  print()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lmao, it's so cute~\n",
      "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5994}\n",
      "\n",
      "Overall, the review is rated as: \n",
      "Positive (Label: 1)\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "im very disappointed\n",
      "{'neg': 0.629, 'neu': 0.371, 'pos': 0.0, 'compound': -0.5256}\n",
      "\n",
      "Overall, the review is rated as: \n",
      "Negative (Label: 0)\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "The price is good but the quality is trash!\n",
      "{'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'compound': 0.3054}\n",
      "\n",
      "Overall, the review is rated as: \n",
      "Positive (Label: 1)\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Lmao, it's so cute~\"\n",
    "text2 = \"im very disappointed\"\n",
    "text3 = \"The price is good but the quality is trash!\"\n",
    "\n",
    "sentiment_analyzer_scores(text1)\n",
    "sentiment_analyzer_scores(text2)\n",
    "sentiment_analyzer_scores(text3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=> Original Review:\")\n",
    "# sentiment_analyzer_scores(dataTrain.iloc[500, 0])\n",
    "# print(\"Actual Label: \", dataTrain.iloc[500, 1])\n",
    "# print(df1.iloc[0, 1])\n",
    "# print(\"=> Sentiment Analysis of the Original Review:\")\n",
    "# print(vaderAnalyzer.polarity_scores(df1.iloc[0, 1]))\n",
    "# print()\n",
    "\n",
    "# print(\"=> Cleaned Text of the review:\")\n",
    "# sentiment_analyzer_scores(reviewCorpus[500])\n",
    "# # print(reviewCorpus[0])\n",
    "# # print(\"=> Sentiment Analysis of the Cleaned Text:\")\n",
    "# # print(vaderAnalyzer.polarity_scores(reviewCorpus[0]))\n",
    "\n",
    "# print(\"Actual Label: \",labelArray[500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the function for VADER analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaderAnalyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define the function for analyzing sentiment using Vader Lexicon:\n",
    "def analyze_sentiment_label(text):\n",
    "    sentimentAnalysisResult = []\n",
    "\n",
    "    for i in range(0, len(text)):\n",
    "        score = vaderAnalyzer.polarity_scores(text[i])\n",
    "        \n",
    "        if score['compound'] <= -0.05:\n",
    "            sentimentAnalysisResult.append(int(0))   # Negative Reviews\n",
    "        elif score['compound'] >= 0.05:\n",
    "            sentimentAnalysisResult.append(int(1))   # Positive Reviews\n",
    "        else:\n",
    "            sentimentAnalysisResult.append(int(2))   # Neutral Reviews (Do not exist in the original dataset)\n",
    "            \n",
    "    return sentimentAnalysisResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER sentiment analysis on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Covert the panda.series to numpy array datatype with \".values\":\n",
    "reviewTrain = dataTrain['text'].values\n",
    "\n",
    "print(len(reviewTrain))\n",
    "print(type(reviewTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vader analysis results:\n",
    "vaderLabels_train = analyze_sentiment_label(reviewTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vaderLabels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[23596 34030  2483]\n",
      " [ 3238 55846   807]\n",
      " [    0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(dataTrain['label'].tolist(), vaderLabels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Data: 66.20166666666667%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Training Data: \" + str(accuracy_score(dataTrain['label'].tolist(), vaderLabels_train) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER sentiment analysis on the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Covert the panda.series to numpy array datatype with \".values\":\n",
    "reviewTest = dataTest['text'].values\n",
    "\n",
    "print(len(reviewTest))\n",
    "print(type(reviewTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Vader analysis results:\n",
    "vaderLabels_test = analyze_sentiment_label(reviewTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vaderLabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 7911 11438   771]\n",
      " [ 1108 18505   267]\n",
      " [    0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(dataTest['label'].tolist(), vaderLabels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Testing Data: 66.03999999999999%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on Testing Data: \" + str(accuracy_score(dataTest['label'].tolist(), vaderLabels_test) * 100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_clean.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
